# 生成AIの不確実性に立ち向かうソフトウェアアーキテクチャ

はじめまして、株式会社Algomaticで、シゴラクAIカンパニーのCTOを務めております菊池です。「生成AIの価値をすべてのデスクワーカーに届ける」というミッションを掲げ、「シゴラクAI」というプロダクトを開発・提供しています。本稿は、プロダクトの運用を通じて見えてきた「生成AI特有の難しさ」と、その難しさにソフトウェア設計で立ち向かった記録となります。

## 法人向け「ChatGPT」というプロダクトを開発して見えてきたもの
シゴラクAIは、いわゆる「法人向けChatGPT」と呼ばれるカテゴリの製品。もう少し広くとらえると、「生成AIを活用したBtoBの生産性向上支援サービス」という立ち位置のプロダクトになります。そんなプロダクトを運用してきて、生成AI特有の不確実性がいくつか見えてきました。

1. 技術的不確実性
2. ユースケースの不確実性
3. 法的不確実性

### 技術的不確実性

技術的不確実性は、シンプルに「生成AI技術のアップデートが早い」という点です。
生成AIをプロダクトに組み込む際に、私たちエンジニアが頭を悩ます要素として、「利用コスト」「回答速度」「コンテキスト長」などがあります。

- コストを下げるために安価なモデルを利用する、自前でLLMを構築する、
- 回答速度の問題を解決するためにキャッシュ処理を工夫したり、遅さが気にならないように体験を工夫したり
- コンテキスト長の課題を克服するためにテキストを分割したり、要約をしたり

こんなさまざまな工夫・努力をするわけですが、半年ごとにはその工夫が不要になっているかもしれない...ということが頭をよぎります。

また、開発プロセスに変化をもたらすようなツール、周辺サービスも日々登場しています。代表的なところではMicrosoft社のPromptFlowがあります。
PromptFlowは、生成AI呼び出しに関わるアルゴリズムを、Azure上でグラフィカルに構築でき、さらに評価やデプロイまでも一気通貫で行えるサービスです。
これにより、RAG（Retrieval Argumented Generation）などのアルゴリズムは、アプリケーションコードではなくPromptFlow上で実装することが可能になります。

生成AIに関連した代表的なライブラリであるLangChainも日々進化しています。最近だとLangGraphというフレームワークが気になっていたりします。

### ユースケースの不確実性
これが最大の不確実要素です。「生成AIをどう活用する？」といったユースケースがそもそも不確実であり、わたしたちAlgomaticも含め、各社模索中です。
シゴラクAIは「法人向けChatGPT」というカテゴリのプロダクトではありますが、「チャットUIが最適だろうか？」という問いは、常にわたしたちも持ち続けています。
カンパニーのミッションを見据えるならば、チャットUI以外の可能性を模索する可能性は大いにあり得ます。

### 規制など法的不確実性
生成AIをめぐっては、データプライバシーや著作権という問題も無視できません。特にトレーニングデータの著作権や生成物に関する法的側面などは各所で議論が進行中であり、
国内でも「生成AIと知財」をめぐるリスク、懸念について目下議論が行われています。今後、精製物に関してさらなるガイドラインが策定される可能性も考えられます。

また、規制とはやや異なりますが、「日本国外に情報を置きたくない」「AIに機密データを学習されては困る」など、生成AIの活用に懸念を示されることも多いです。
技術の黎明期は、こういった判断において振れ幅が大きい。


## シゴラクAIにとって最適なアーキテクチャを考える
大前提として、設計に正解はない。「シゴラクAI事業として大切にしているものは何か」という点を最大限に考慮することが大事。
シゴラクAIはまだまだできたばかりのプロダクトであり、「法人向けChatGPT」という形すら今後変わりうる。

以下のような価値検証を行なっていくことが大事だと考えている。
- チャットUIという枠を超えた新たな体験の創出
- 回答エンジンの新規実装や改良を重ねる
- 特定の実装技術には依存しない
→ コアドメインを互いに分離する。

一方で、経験的に特定の生成AIサービス、モデルに依存することもリスクが高い。
柔軟な切り替えが可能であることの価値が高いと考えた。

→ モデルを容易に切り替え可能とする。

## シゴラクAIのコアドメインは「ユーザー体験」と「回答エンジン」
コアドメインは、UIも含め「ユーザーとAIアシスタントとの対話」の体験。
そのため、UIは柔軟に変更したい。裏側のロジックや事情に左右されることはよくない。
一方で、AIの回答エンジンはUIに依らず柔軟に開発をしたい。また、回答エンジン同士も互いに無関係であることが望ましい。
そこで、AIとユーザーのやり取りを「ChatEngine」として抽象化し、UIと各エンジンは互いに依存せず、抽象にのみ依存する設計とした。

```
ChatEngineのインターフェースを貼る
```


## モデルを容易に切り替え可能とする
LLMの呼び出し層を「LLM Gateway」として抽象化した。

- 呼び出し先がAzure OpenAI Serviceであれば、Quota制限緩和のために複数エンドポイントの呼び分けをする必要があったりする。
- また、「LLMの回答をJSON形式に固定する」ような、特定のモデルにしか存在しない機能などもある。

これらをコアロジックに染み出させたくない。
さらに、コストや速度面を意識したチューニングにおいては、呼び出し先のモデルを手軽に切り替えて動作テストを行いたくなることもある。

```
LLM Gatewayのインターフェースを貼る
```

## クリーンアーキテクチャではない？
明確にレイヤーの定義こそしていないが、依存の方向を制限してコアドメインを依存関係の頂点に置く、という考え方はクリーンアーキテクチャと同様。
現在シゴラクAIはコードベースもさほど大きくなく、開発チームも少人数で取り組んでいるため、軽量な設計が最も適していると判断している。




## ChatEngine
シゴラクAIのメイン機能は、ユーザーとAIとの対話である。

具体的にシゴラクAIでは、主に以下のような種類のチャット機能がある。
- 通常のチャット
- ナレッジチャット
- WebQ&A

通常のチャットは、シンプルにユーザからの発話をLLMに渡して、回答を返すのみ。
ナレッジチャットは、いわゆるRAG（Retrieval Argumented Generation）と呼ばれる、社内ドキュメントに基づいた回答を返すもの。
WebQ&Aは、ユーザーの発話に含まれるWebサイトの内容を取得し、その内容に基づいて回答を返すもの。

シンプルな実装としては、ユーザーからの発話を受けて、以下のような分岐をすることが考えられる。

```
if (has_document) {
    // ナレッジチャット
    ...
} else if (has_url) {
    // WebQ&A
    ...
} else {
    // 通常のチャット
    ...
}
```




〜〜〜〜〜〜

# 生成AIの不確実さに立ち向かうLLMエンジニアリング

「生成AIによる業務効率化SaaS」であるシゴラクAIでは、生成AIを多く利用します

生成AIのプロダクト活用における観点は以下の通り。
- LLMによる回答品質はどうか？
- 速度は早い？
- コストはいくら？
- コンテキスト長はどこまで利用可能？

コンテキスト長：生成AIに対して一度に渡せるトークン（文字列長のようなもの）の長さ。長ければ長いほど多くの情報量を渡すことができる。
仕様として長さが決まっていることが多い。一方で、長大なコンテキストを受け入れるモデルであっても、それによって回答品質が低下することもある。

2023年初期にリリースされ、広く使われているGPT-4は、旧モデルであえるGPT-3.5と比較すると回答品質は高いが速度は遅く、コストも高い。コンテキスト長も長くはない。


